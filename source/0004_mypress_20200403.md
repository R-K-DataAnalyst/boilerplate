# 遊びコード群一時避難

```python:kiroku_you.py
# パッケージインポート
import numpy as np
import scipy as sp
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pandas.plotting import register_matplotlib_converters
import datetime as dt
import dateutil
from dateutil.relativedelta import relativedelta
import xgboost
import xgboost.sklearn as xgb
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import auc
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_validate
import pickle
import optuna
import functools
import time
register_matplotlib_converters()
sns.set()

# メモリ削減関数
def reduce_mem_usage(df, verbose=True):
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage().sum() / 1024**2    
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)    
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
    return df


# 交差検証用にtrainデータをtrainとvalidに分割する関数

def createXy(df, target_meter=0, test_size=0.3, random_state=0, future=3, plot=False):
    '''
    target_meter:
    future:
    '''
    if future > 3:
        raise ValueError("future is <=3 !")
    else:
        # 説明変数と目的変数の分離
        dfx=df[df['user_class']==target_meter][feature]
        dfy=df[df['user_class']==target_meter][target[future-1]]
        dfx = dfx.drop("user_class", axis=1)
        
        # trainとvalidに分割
        X_train, X_test, y_train, y_test = train_test_split(dfx, dfy, test_size=test_size, random_state=random_state)
        print('Taget Class is '+str(target_meter))
        print('TrainX Size is {}'.format(X_train.shape))
        print('TestX Size is {}'.format(X_test.shape))
        print('TrainY Size is {}'.format(y_train.shape))
        print('TestY Size is {}'.format(y_test.shape))
        return X_train, y_train, X_test, y_test


# 主成分分析の結果を第3主成分まで変数に追加する関数

def make_pca(X_train, X_test, plot=False):
    # データの標準化処理
    sc = StandardScaler()
    sc.fit(X_train)
    X_train_std = sc.transform(X_train)
    sc.fit(X_test)
    X_test_std = sc.transform(X_test)

    # PCA
    N = 10
    pca = PCA(n_components=N)
    pca.fit(X_train_std)
    
    # データセットを主成分に変換する:train
    transformed_train = pca.fit_transform(X_train_std)
    dfs = pd.DataFrame(transformed_train, columns=["PC{}".format(x + 1) for x in range(N)])
    if plot==True:
        display(pd.DataFrame(np.cumsum(pca.explained_variance_ratio_*100.), index=["PC{}".format(x + 1) for x in range(len(dfs.columns))],columns=['寄与率']))

    # データセットを主成分に変換する:valid
    transformed_test = pca.fit_transform(X_test_std)
    dfs = pd.DataFrame(transformed_test, columns=["PC{}".format(x + 1) for x in range(N)])
    if plot==True:
        display(pd.DataFrame(np.cumsum(pca.explained_variance_ratio_*100.), index=["PC{}".format(x + 1) for x in range(len(dfs.columns))],columns=['寄与率']))
        print(X_train.shape)
        print(X_train_std.shape)
        print(transformed_train.shape)
    
    # 第3主成分まで変数に追加
    X_train['pca01']=transformed_train[:,0]
    X_train['pca02']=transformed_train[:,1]
    X_train['pca03']=transformed_train[:,2]

    X_test['pca01']=transformed_test[:,0]
    X_test['pca02']=transformed_test[:,1]
    X_test['pca03']=transformed_test[:,2]
    
    return X_train, X_test




# Optunaによるhypyer param最適化のための目的関数
def objective(X_train, y_train, X_test, y_test, trial):
    '''
    trial:set of hyperparameter    
    '''
    # hypyer param
    # ==== #
    max_depth = trial.suggest_int('max_depth', 3, 30)
    n_estimators = trial.suggest_int('n_estimators', 2, 300)
    subsample = trial.suggest_loguniform('subsample', 1e-6, 1.0)
    #learning_rate = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)
    #colsample_bytree = trial.suggest_loguniform('colsample_bytree', 1e-8, 1.0)
    min_child_weight = trial.suggest_int('min_child_weight', 1, 20)
    # ==== #
 
    # model
    model = xgb.XGBClassifier(max_depth=max_depth\
                              ,n_estimators=n_estimators\
                              ,subsample=subsample\
                              ,learning_rate=0.1\
                              #,colsample_bytree=colsample_bytree\
                              ,min_child_weight=min_child_weight\
                              ,verbose=3)
    
    # 3-Fold CV / F-1 score でモデルを評価する
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)
    scores = cross_validate(model, X_train, y_train, cv=skf, scoring='f1')
 
    # eval
    score = scores['test_score'].mean()
    return score



#Train Validデータ作成
X_train, y_train, X_test, y_test = createXy(df3, target_meter=i+1,future=future)
# 主成分を変数に追加
X_train, X_test=make_pca(X_train, X_test)
    
# 交差検証のためのデータ取得数
kf = KFold(n_splits=nn, shuffle=True)
cv=kf

study = optuna.create_study(direction='maximize')
study.optimize(func=functools.partial(objective, X_train, y_train, X_test, y_test)\
               ,n_trials=50\
               ,timeout=90\
               ,n_jobs=-1)

#最適化したハイパーパラメータの確認
print('== Result ==')
print('====================')
print('best_param:{}'.format(study.best_params))
print('====================')
 
#最適化後の目的関数値
print('best_value:{}'.format(study.best_value))
print('====================')
 
#最適な試行
print('best_trial:{}'.format(study.best_trial))
print('====================')
 
# トライアルごとの結果を確認
for j in study.trials:
	print('param:{0}, eval_value:{1}'.format(j.params, j.value))
print('====================')

    
best = xgb.XGBClassifier(**study.best_params)
best.fit(X_train, y_train)
    
# モデルを保存する
filename = 'class0'+str(i+1)+'_'+title_name+'_xgb_after0'+str(future)+'.sav'
pickle.dump(best, open(filename, 'wb'))
    
print('#### End Learning ####')
    
tuna_pred_test = best.predict(X_test)
print('Best F1_score for Test Data: ',f1_score(y_test, tuna_pred_test))  # 最も良かったスコア
print('')



start = time.time()

# 保存したモデルをロードする

class_num=len(df3['user_class'].unique()) # クラスの数
models123=[] # 読み込んだモデルを入れる空のリスト

for j in futures:
    models=[]
    for i in range(class_num):
        filename = 'class0'+str(i+1)+'_'+title_name+'_xgb_after0'+str(j)+'.sav'
        loaded_model = pickle.load(open(filename, 'rb'))
        X_train, y_train, X_test, y_test=createXy(df3, target_meter=i+1)
        X_train, X_test=make_pca(X_train, X_test)
        result = loaded_model.score(X_test, y_test)
        models.append(loaded_model)
        print(result)
    
        test_pred = loaded_model.predict(X_test)
        learn_pred = loaded_model.predict(X_train)
        test_real = y_test
        learn_real = y_train
    
        result_train=pd.DataFrame()
        result_test=pd.DataFrame()
        result_train['train_real']=learn_real
        result_train['train_pred']=0
        result_test['test_real']=test_real
        result_test['test_pred']=0
        result_train['train_pred']=learn_pred
        result_train['flg']=1
        result_test['test_pred']=test_pred
        result_test['flg']=1
    
        display(pd.pivot_table(result_train, values='flg', index='train_real',columns='train_pred',aggfunc='count'))
        display(pd.pivot_table(result_test, values='flg', index='test_real',columns='test_pred',aggfunc='count'))
        print('Predict - Real ratio:', result_test['test_pred'].sum() / result_test['test_real'].sum())
        print('Predict - Real diff:', result_test['test_pred'].sum() - result_test['test_real'].sum())
        print('')
    models123.append(models)
    
elapsed_time = time.time() - start
print ("elapsed_time:{0}".format(elapsed_time) + "[sec]")



## 評価 ##
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import auc
from sklearn.metrics import roc_curve
from sklearn.metrics import classification_report

df_cnt=len(result)
print(title_name)
for i in range(df_cnt):
    print('######## Evaluation; Data {} ########'.format(i+1))
    # 混合行列
    cm=pd.crosstab(index=result[i]['test_real'],columns=result[i]['test_pred'],values=result[i]['flg'],aggfunc='count')
    display(cm)
    # 各評価指標
    y_true = result[i]['test_real'].values
    y_pred = result[i]['test_pred'].values
    y_pred_proba = result[i]['test_pred_proba'].values
    print('pred / real',y_pred.sum()/y_true.sum())
    print('pred - real',y_pred.sum()-y_true.sum())
    print('accuracy_score',accuracy_score(y_true, y_pred))
    print('precision_score',precision_score(y_true, y_pred))
    print('recall_score',recall_score(y_true, y_pred))
    print('f1_score',f1_score(y_true, y_pred))
    display(pd.DataFrame(classification_report(y_true, y_pred, output_dict=True)))

    # FPR, TPR、しきい値を算出
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    # ついでにAUCも
    auc_ = auc(fpr, tpr)
    # ROC曲線をプロット
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %.2f)'%auc_)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.legend()
    plt.title('ROC curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.grid(True)
    plt.show()
    print('####################################')
    print('')



# plot用データフレーム作成関数
def count_make(result,watch,cnt,yyyymm=201911):
    for i in range(len(result)):
        count1=pd.DataFrame(result[i].groupby(['flg',watch])[cnt].count()).reset_index().rename(columns={cnt:'UU'})
        count2=pd.DataFrame(result[i].groupby(['flg'])[cnt].count()).reset_index().rename(columns={cnt:'AllUU'})
        count3=pd.merge(count1,count2,on=['flg'],how='left')
        count3['ratio']=count3['UU']/count3['AllUU']
        count3['month']=dt.datetime(int(str(yyyymm)[:4]),int(str(yyyymm)[4:]),1) + relativedelta(months=i)
        if i == 0:
            count=pd.DataFrame(count3)
        else:
            count=pd.concat([count, count3])
    return count



# plot用関数
def count_plot(count,watch,cnt,xlim=None,ylim=None):
    bottom=0
    count['month']=count['month'].astype(str)
    
    fig=plt.figure(figsize=(10,5))
    sns.set()
    ax1=plt.subplot(1,1,1)
    flg_num=count['flg'].unique()
    for i in range(len(flg_num)):
        try:
            ax1.bar(count[(count[watch]==1)&(count['flg']==flg_num[i])]['month']\
                    ,count[(count[watch]==1)&(count['flg']==flg_num[i])]['UU']\
                    ,color="C"+str(flg_num[i]),label='UU_continues'+str(flg_num[i]),bottom=bottom)
            bottom += count[(count[watch]==1)&(count['flg']==flg_num[i])]['UU'].values
        except ValueError:
            print('Not Exist Continuing users flg_num:{} month:{}'.format(flg_num[i],count[(count[watch]==1)&(count['flg']==flg_num[i])]['month'].unique()))
            bottom += 0
    
    ax1.set_xlabel('# Class')
    ax1.set_ylabel('# continues_UU')
    ax1.legend(loc='upper right',bbox_to_anchor=(1.2, 1))
    ax1.set_title(watch)
    if xlim is not None:
        ax1.set_xlim(xlim[0],xlim[1])
    if ylim is not None:
        ax1.set_ylim(ylim[0],ylim[1])
    
    plt.show()
```
